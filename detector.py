#!/usr/bin/env python3
"""
Bot or Not Challenge â€” Bot Detection Pipeline
=============================================
A multi-tier scoring system that identifies bot accounts in social media datasets.

Each user is scored across multiple tiers of signals. Users whose total score
meets or exceeds the DETECTION_THRESHOLD are flagged as bots.

Scoring rationale (competition: +4 TP, -1 FN, -2 FP):
  - False positives are costly (2x a missed bot), so we require converging signals.
  - Tier 1 signals are so reliable they can flag independently.
  - Tier 2/3 signals require combinations to reach the threshold.

"""

import json
import re
import sys
import string
import statistics
import argparse
from collections import defaultdict, Counter
from datetime import datetime
from pathlib import Path


# ============================================================================
# CONFIGURATION
# ============================================================================

DETECTION_THRESHOLD = 3  # Minimum score to flag as bot (tuned on practice data)

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"

DATASET_POSTS_USERS_30 = DATA_DIR / "dataset.posts&users.30.json"
DATASET_POSTS_USERS_31 = DATA_DIR / "dataset.posts&users.31.json"
DATASET_POSTS_USERS_32 = DATA_DIR / "dataset.posts&users.32.json"
DATASET_POSTS_USERS_33 = DATA_DIR / "dataset.posts&users.33.json"

DATASET_BOTS_30 = DATA_DIR / "dataset.bots.30.txt"
DATASET_BOTS_31 = DATA_DIR / "dataset.bots.31.txt"
DATASET_BOTS_32 = DATA_DIR / "dataset.bots.32.txt"
DATASET_BOTS_33 = DATA_DIR / "dataset.bots.33.txt"


def _resolve_dataset_alias(dataset_arg):
    """Resolve dataset aliases ('30'/'32') or return provided path as-is."""
    aliases = {
        "30": DATASET_POSTS_USERS_30,
        "31": DATASET_POSTS_USERS_31,
        "32": DATASET_POSTS_USERS_32,
        "33": DATASET_POSTS_USERS_33,
    }
    if dataset_arg in aliases:
        return str(aliases[dataset_arg])
    return dataset_arg


def _default_bots_for_dataset(dataset_path):
    """Pick default bots file when dataset is one of the known challenge files."""
    name = Path(dataset_path).name
    if name == "dataset.posts&users.30.json":
        return str(DATASET_BOTS_30)
    if name == "dataset.posts&users.32.json":
        return str(DATASET_BOTS_32)
    return None


def _dataset_id_from_path(dataset_path):
    """Infer challenge dataset id (30/32) from filename when possible."""
    name = Path(dataset_path).name
    if name == "dataset.posts&users.30.json":
        return "30"
    if name == "dataset.posts&users.30.json":
        return "31"
    if name == "dataset.posts&users.32.json":
        return "32"
    if name == "dataset.posts&users.30.json":
        return "33"
    return "custom"


def _output_path_for_dataset(base_output, dataset_path):
    """Create dataset-specific output file when processing multiple datasets."""
    dataset_id = _dataset_id_from_path(dataset_path)
    output = Path(base_output)
    if output.suffix:
        return str(output.with_name(f"{output.stem}.{dataset_id}{output.suffix}"))
    return str(output.with_name(f"{output.name}.{dataset_id}.txt"))


# ============================================================================
# FEATURE EXTRACTION
# ============================================================================

def parse_timestamps(posts):
    """Parse ISO timestamps from posts into datetime objects."""
    times = []
    for p in sorted(posts, key=lambda x: x["created_at"]):
        t = p["created_at"].replace("Z", "+00:00")
        times.append(datetime.fromisoformat(t))
    return times


def compute_intervals(times):
    """Compute inter-tweet intervals in seconds."""
    return [(times[i + 1] - times[i]).total_seconds() for i in range(len(times) - 1)]


def compute_cv(intervals):
    """Compute coefficient of variation of intervals (std / mean)."""
    if len(intervals) < 3:
        return None  # Not enough data
    mean = statistics.mean(intervals)
    if mean == 0:
        return 0.0
    return statistics.stdev(intervals) / mean


# ============================================================================
# TIER 1 â€” NEAR-CERTAIN SIGNALS (10 points each)
# ============================================================================
# These signals have essentially zero false positive risk. Any one of them
# alone is enough to confidently flag a bot.

# --- Tier 1a: Meta-Text / Leaked LLM Prompts ---
# Some bots are generated by LLMs that leak their instructions into the output.
# E.g., "Here are some of my recent tweets:" or "Here's a revised version."
# Real humans never write tweets like this.

META_PHRASES = [
    "here are some of my recent tweets",
    "here are my recent tweets",
    "here are some of my latest tweets",
    "here are some recent rewrites",
    "here are the revised versions",
    "here are some modified versions",
    "here are some alternatives",
    "here are some rewrites",
    "here are some changes",
    "here's a slightly modified version",
    "here's a slightly altered version",
    "here's a minor revision",
    "here's a minor revised version",
    "here's a minor re-phrased version",
    "here's a minor change",
    "here's a lightly rephrased version",
    "here's a subtle rewrite",
    "here's the revised version",
    "here's a stat for you",  # Common bot copy pattern
    "revised version of the",
    "modified version of",
    "slightly modified version",
    "some of my recent tweets",
    "my recent tweets",
    "rewritten with minor changes",
    "content generation assistant",
    "you are a content generation assistant",
    "you are trained on data up to",
]


def tier1a_meta_text(texts):
    """
    TIER 1a: Leaked LLM prompt / meta-text detection.
    
    Scans all tweets for phrases that indicate the text was generated by an LLM
    that leaked its system prompt or instruction framing into the output.
    
    Returns: (score, count of meta-text matches)
    """
    count = 0
    for text in texts:
        text_lower = text.lower()
        for phrase in META_PHRASES:
            if phrase in text_lower:
                count += 1
                break  # One match per tweet is enough
    
    # 2+ matches = very confident (10 pts)
    # 1 match could be coincidence, give partial credit (2 pts)
    if count >= 2:
        return 10, count
    elif count == 1:
        return 2, count
    return 0, count


# --- Tier 1b: Encoding Artifacts / Garbled Text ---
# Some bot generation pipelines produce text with control characters or
# encoding corruption (e.g., \x00-\x1f bytes). Real scraped tweets from
# Twitter's API never contain these. Zero false positives observed.

GARBLED_PATTERN = re.compile(r"[\x00-\x08\x0b\x0c\x0e-\x1f]")


def tier1b_encoding_artifacts(texts):
    """
    TIER 1b: Encoding artifact detection.
    
    Checks for control characters in tweet text that indicate corrupted
    text generation pipeline output.
    
    Returns: (score, count of garbled tweets)
    """
    count = sum(1 for t in texts if GARBLED_PATTERN.search(t))
    
    if count >= 1:
        return 10, count
    return 0, count


# ============================================================================
# TIER 2 â€” STRONG SIGNALS (3-5 points each)
# ============================================================================
# These signals have low but non-zero false positive risk. They are strong
# enough to contribute significantly to the score, but typically need at
# least one other signal to reach the threshold.

# --- Tier 2a: Same-Second Tweet Bursts ---
# Bots often dump multiple tweets at the exact same timestamp (same second),
# which happens when a script generates and posts them in a batch. Real users
# occasionally have a few (thread posts, API imports), but bots have many more.

def tier2a_same_second_bursts(posts):
    """
    TIER 2a: Same-second tweet burst detection.
    
    Counts how many tweets share an exact timestamp (to the second).
    Bots that batch-post will have many; humans rarely exceed 2-3.
    
    Returns: (score, count of duplicate-second tweets)
    """
    time_counts = Counter(p["created_at"] for p in posts)
    same_second = sum(v - 1 for v in time_counts.values() if v > 1)
    
    if same_second >= 5:
        return 5, same_second
    elif same_second >= 3:
        return 3, same_second
    return 0, same_second


# --- Tier 2b: Posting Interval Regularity (CV) ---
# The coefficient of variation (std/mean) of inter-tweet time gaps.
# Humans post erratically (bursts then silence) â†’ high CV (~1.8-2.0).
# Bots post more evenly â†’ low CV (~1.0-1.3).
# This is the single strongest statistical signal across both practice datasets.

def tier2b_interval_regularity(times):
    """
    TIER 2b: Posting interval regularity via coefficient of variation.
    
    Low CV = suspiciously regular posting pattern.
    Requires â‰¥15 posts â€” with fewer tweets, CV is too noisy and causes
    false positives on low-volume human accounts.
    
    Thresholds tuned on practice data:
      CV â‰¤ 0.8  â†’ very regular (5 pts)
      CV â‰¤ 1.0  â†’ regular (4 pts)
      CV â‰¤ 1.15 â†’ somewhat regular (3 pts)
    
    Returns: (score, cv_value or None)
    """
    if len(times) < 15:
        return 0, None  # Not enough posts for reliable CV
    
    intervals = compute_intervals(times)
    cv = compute_cv(intervals)
    
    if cv is None:
        return 0, None
    
    if cv <= 0.8:
        return 5, cv
    elif cv <= 1.0:
        return 4, cv
    elif cv <= 1.15:
        return 3, cv
    return 0, cv


# --- Tier 2c: Template Pattern (Zero URL + Zero Hashtag) ---
# A specific class of bots generates tweets from fill-in-the-blank templates
# (e.g., "The ending of [MOVIE] still messes me up."). These bots produce
# pure text with no URLs or hashtags, and at decent volume. Real users who
# tweet 15+ times almost always include at least some URLs or hashtags.

def tier2c_template_pattern(texts, n_posts):
    """
    TIER 2c: Template bot detection via zero-URL + zero-hashtag pattern.
    
    Catches bots that generate text from templates without any links or tags.
    Requires â‰¥30 posts â€” at lower volumes, some humans naturally don't
    include URLs or hashtags, causing false positives.
    
    Returns: (score, is_template_pattern: bool)
    """
    url_rate = sum(1 for t in texts if "http" in t) / max(len(texts), 1)
    hashtag_rate = sum(t.count("#") for t in texts) / max(len(texts), 1)
    
    if url_rate == 0 and hashtag_rate == 0 and n_posts >= 30:
        return 5, True
    return 0, False


# --- Tier 2d: High 'just' Frequency (Quirky Anecdote Bots) ---
# A class of bots generates relatable anecdote tweets that heavily overuse
# the word "just" â€” "just walked into a glass door", "just waved at someone",
# "just accidentally liked a photo from 5 years ago". Humans use "just" at
# ~5-6% of tweets; these bots hit 35-60%. At â‰¥0.35 with â‰¥15 posts, this
# signal has ZERO false positives across both practice datasets.

def tier2d_just_frequency(texts, n_posts):
    """
    TIER 2d: Overuse of 'just' indicating quirky-anecdote bot pattern.
    
    Requires â‰¥15 posts for reliability. Zero FP risk observed.
      'just' rate â‰¥ 0.35 â†’ 4 pts
    
    Returns: (score, just_rate)
    """
    if n_posts < 15:
        return 0, 0.0
    
    just_count = sum(1 for t in texts if re.search(r'\bjust\b', t.lower()))
    just_rate = just_count / n_posts
    
    if just_rate >= 0.35:
        return 4, just_rate
    return 0, just_rate


# --- Tier 2e: Zero-Engagement Pattern (No URLs + No Mentions) ---
# Real Twitter users almost always interact â€” sharing links, replying to
# people, or mentioning accounts. Bots that generate synthetic content
# into the void have zero URLs AND zero @mentions. This is a broader
# version of the template check (Tier 2c) that catches bots which DO
# use hashtags but never engage with real content or people.

def tier2e_zero_engagement(texts, n_posts):
    """
    TIER 2e: Zero-engagement pattern â€” no URLs and no @mentions.
    
    Catches bots that generate content but never link to anything or
    interact with other accounts. Requires â‰¥15 posts.
      url_rate=0 AND mention_rate=0 â†’ 3 pts
    
    Returns: (score, is_zero_engagement: bool)
    """
    if n_posts < 15:
        return 0, False
    
    url_rate = sum(1 for t in texts if "http" in t) / n_posts
    mention_rate = sum(1 for t in texts if "@" in t) / n_posts
    
    if url_rate == 0 and mention_rate == 0:
        return 2, True
    return 0, False


# ============================================================================
# TIER 3 â€” SUPPORTING SIGNALS (1-2 points each)
# ============================================================================
# Weak signals that add evidence when combined with other tiers.
# Never enough to flag a bot alone, but push borderline cases over the edge.

# --- Tier 3a: Elevated Hashtag Rate ---
# Bots average ~0.9 hashtags/tweet vs ~0.2 for humans.
# High hashtag density suggests automated content seeding.

def tier3a_hashtag_density(texts, n_posts):
    """
    TIER 3a: Hashtag density analysis.
    
    Bots tend to use significantly more hashtags than humans.
      â‰¥1.0 hashtags/tweet â†’ 2 pts
      â‰¥0.5 hashtags/tweet â†’ 1 pt
    
    Returns: (score, hashtag_rate)
    """
    if n_posts < 20:
        return 0, 0.0

    hashtag_rate = sum(t.count("#") for t in texts) / max(len(texts), 1)
    if hashtag_rate >= 1.2:
        return 2, hashtag_rate
    elif hashtag_rate >= 0.8:
        return 1, hashtag_rate
    return 0, hashtag_rate

# --- Tier 3b: Very Low URL Rate ---
# Real users share links frequently (~52% of tweets contain URLs).
# Bots that generate original text rarely include real URLs (~29%).
# Very low URL rate at decent volume is suspicious.

def tier3b_low_url_rate(texts, n_posts):
    """
    TIER 3b: Unusually low URL sharing rate.
    
    Most humans share links regularly. Bots generating synthetic content
    rarely include URLs. Requires â‰¥15 posts to be meaningful.
      URL rate â‰¤ 10% with â‰¥15 posts â†’ 1 pt
    
    Returns: (score, url_rate)
    """
    url_rate = sum(1 for t in texts if "http" in t) / max(len(texts), 1)
    
    if url_rate <= 0.10 and n_posts >= 15:
        return 1, url_rate
    return 0, url_rate


# --- Tier 3c: 'Fun Fact' Pattern ---
# Quirky-anecdote bots frequently inject "fun fact:" into their tweets
# as a filler device. E.g., "walked into a glass door. fun fact: birds
# do it too." Zero humans in practice data use this phrase 2+ times.

FUN_FACT_PATTERN = re.compile(r'\bfun fact\b', re.IGNORECASE)


def tier3c_fun_fact(texts):
    """
    TIER 3c: Repeated 'fun fact' usage.
    
    Quirky-anecdote bots use this phrase as a filler device. Humans
    almost never use it more than once.
      'fun fact' count â‰¥ 2 â†’ 2 pts
    
    Returns: (score, fun_fact_count)
    """
    count = sum(1 for t in texts if FUN_FACT_PATTERN.search(t))
    
    if count >= 2:
        return 2, count
    return 0, count


# --- Tier 3d: Repetitive Tweet Opener ---
# Some bots start many tweets with the same phrase, revealing a template.
# E.g., "Remember when..." appearing as the opener in 15/36 tweets.
# Humans don't repeat opening phrases this aggressively.

REPETITIVE_OPENERS = [
    "remember when",
    "not gonna lie",
    "can we talk about",
    "is it just me or",
    "unpopular opinion",
]


def tier3d_repetitive_opener(texts):
    """
    TIER 3d: Repetitive tweet opener detection.
    
    Checks if any single opening phrase appears in â‰¥3 tweets, indicating
    a template-driven generation pattern.
      Any opener repeated â‰¥ 3 times â†’ 2 pts
    
    Returns: (score, most_repeated_opener_count)
    """
    max_count = 0
    for opener in REPETITIVE_OPENERS:
        count = sum(1 for t in texts if t.lower().strip().startswith(opener))
        max_count = max(max_count, count)
    
    if max_count >= 3:
        return 2, max_count
    return 0, max_count

def _normalize_text(t: str) -> str:
    """Lowercase, remove punctuation, collapse spaces (for duplicate detection)."""
    t = t.lower()
    t = t.translate(str.maketrans("", "", string.punctuation))
    t = re.sub(r"\s+", " ", t).strip()
    return t


def tier2f_duplicate_text(texts, n_posts):
    """
    TIER 2f: Duplicate / template tweet detection.

    If a user repeats the same (normalized) tweet many times, it's very bot-like.
    Requires >=15 posts to avoid small-sample false positives.

    Returns: (score, max_duplicate_count)
    """
    if n_posts < 15:
        return 0, 0

    normed = [_normalize_text(t) for t in texts]
    counts = Counter(normed)
    max_dup = max(counts.values()) if counts else 0

    if max_dup >= 6:
        return 5, max_dup
    elif max_dup >= 4:
        return 3, max_dup
    return 0, max_dup


# ============================================================================
# PIPELINE: SCORE A SINGLE USER
# ============================================================================

def score_user(uid, posts, verbose=False):
    """
    Run all detection tiers on a single user and return their total score
    along with a breakdown of which tiers fired.
    """
    posts_sorted = sorted(posts, key=lambda x: x["created_at"])
    texts = [p["text"] for p in posts_sorted]
    times = parse_timestamps(posts_sorted)
    n_posts = len(posts_sorted)

    breakdown = {}
    total_score = 0

    # --- Tier 1: Near-certain signals ---
    s, detail = tier1a_meta_text(texts)
    breakdown["T1a_meta_text"] = {"score": s, "meta_matches": detail}
    total_score += s

    s, detail = tier1b_encoding_artifacts(texts)
    breakdown["T1b_encoding"] = {"score": s, "garbled_tweets": detail}
    total_score += s

    # --- Tier 2: Strong signals ---
    s, detail = tier2a_same_second_bursts(posts_sorted)
    breakdown["T2a_same_second"] = {"score": s, "duplicate_timestamps": detail}
    total_score += s

    s, detail = tier2b_interval_regularity(times)
    breakdown["T2b_interval_cv"] = {"score": s, "cv": detail}
    total_score += s

    s, detail = tier2c_template_pattern(texts, n_posts)
    breakdown["T2c_template"] = {"score": s, "is_template": detail}
    total_score += s

    s, detail = tier2d_just_frequency(texts, n_posts)
    breakdown["T2d_just_freq"] = {"score": s, "just_rate": detail}
    total_score += s

    s, detail = tier2e_zero_engagement(texts, n_posts)
    breakdown["T2e_zero_engage"] = {"score": s, "is_zero_engagement": detail}
    total_score += s
    
    s, detail = tier2f_duplicate_text(texts, n_posts)
    breakdown["T2f_dup_text"] = {"score": s, "max_dup": detail}
    total_score += s


    # --- Tier 3: Supporting signals ---
    s, detail = tier3a_hashtag_density(texts, n_posts)
    breakdown["T3a_hashtags"] = {"score": s, "hashtag_rate": detail}
    total_score += s

    s, detail = tier3b_low_url_rate(texts, n_posts)
    # Don't double-count: if T2e (zero engagement) already fired, skip T3b
    # since both measure the same underlying signal (no URLs)
    if breakdown["T2e_zero_engage"]["score"] > 0:
        s = 0
    breakdown["T3b_low_url"] = {"score": s, "url_rate": detail}
    total_score += s

    s, detail = tier3c_fun_fact(texts)
    breakdown["T3c_fun_fact"] = {"score": s, "fun_fact_count": detail}
    total_score += s

    s, detail = tier3d_repetitive_opener(texts)
    breakdown["T3d_rep_opener"] = {"score": s, "opener_count": detail}
    total_score += s

    if verbose:
        flagged = total_score >= DETECTION_THRESHOLD
        status = "ðŸ¤– BOT" if flagged else "   human"
        print(f"  {status} | score={total_score:3d} | {uid[:8]}... | {n_posts:3d} tweets", end="")
        fired = [k for k, v in breakdown.items() if v["score"] > 0]
        if fired:
            print(f" | fired: {', '.join(fired)}", end="")
        print()
        
    tier1_fired = (breakdown["T1a_meta_text"]["score"] > 0) or (breakdown["T1b_encoding"]["score"] > 0)
    tier2_fired = any(v["score"] > 0 for k, v in breakdown.items() if k.startswith("T2"))

    flagged = tier1_fired or (tier2_fired and total_score >= DETECTION_THRESHOLD)


    return total_score, breakdown, flagged


# ============================================================================
# PIPELINE: PROCESS FULL DATASET
# ============================================================================

def detect_bots(dataset_path, threshold=DETECTION_THRESHOLD, verbose=False):
    """
    Load a dataset, score all users, and return the list of detected bot IDs.
    """
    # Load dataset
    print(f"Loading dataset from {dataset_path}...")
    with open(dataset_path, "r", encoding="utf-8-sig") as f:
        data = json.load(f)

    # Extract metadata
    ds_id = data.get("id", "?")
    lang = data.get("lang", "?")
    metadata = data.get("metadata", {})
    total_users = metadata.get("total_amount_users", len(data.get("users", [])))
    total_posts = metadata.get("total_amount_posts", len(data.get("posts", [])))

    print(f"Dataset {ds_id} | Language: {lang}")
    print(f"Users: {total_users} | Posts: {total_posts}")
    print(f"Detection threshold: {threshold}")
    print("-" * 60)

    # Group posts by author
    user_posts = defaultdict(list)
    for p in data["posts"]:
        user_posts[p["author_id"]].append(p)

    # Score each user
    detections = []
    all_scores = {}

    for uid, posts in user_posts.items():
        score, breakdown, flagged = score_user(uid, posts, verbose=verbose)
        all_scores[uid] = (score, breakdown)
        if flagged:
            detections.append(uid)

    # Summary
    print("-" * 60)
    print(f"Detected {len(detections)} bots out of {len(user_posts)} users")
    print(f"Detection rate: {len(detections)/len(user_posts):.1%}")

    # Score distribution
    score_vals = [s for s, _ in all_scores.values()]
    bins = Counter()
    for s in score_vals:
        if s == 0:
            bins["0"] += 1
        elif s < threshold:
            bins[f"1-{threshold-1}"] += 1
        else:
            bins[f">={threshold}"] += 1
    print(f"Score distribution: {dict(bins)}")

    return detections, all_scores


# ============================================================================
# EVALUATION (for practice datasets with known bots)
# ============================================================================

def evaluate(detections, bots_file):
    """
    Evaluate detections against known bot labels.
    Uses the competition scoring: +4 TP, -1 FN, -2 FP.
    """
    with open(bots_file, "r", encoding="utf-8-sig") as f:
        true_bots = set(line.strip() for line in f if line.strip())

    detected = set(detections)
    tp = len(detected & true_bots)
    fp = len(detected - true_bots)
    fn = len(true_bots - detected)
    tn = -1  # We don't track total users here

    score = 4 * tp - 1 * fn - 2 * fp
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / len(true_bots) if true_bots else 0
    max_score = 4 * len(true_bots)

    print("\n" + "=" * 60)
    print("EVALUATION RESULTS")
    print("=" * 60)
    print(f"True bots in dataset:  {len(true_bots)}")
    print(f"Detected as bots:      {len(detected)}")
    print(f"True Positives (TP):   {tp}")
    print(f"False Positives (FP):  {fp}")
    print(f"False Negatives (FN):  {fn}")
    print(f"Precision:             {precision:.1%}")
    print(f"Recall:                {recall:.1%}")
    print(f"Competition Score:     {score:+d} / {max_score} ({score/max_score:.0%} of perfect)")
    print("=" * 60)

    # Show missed bots
    missed = true_bots - detected
    if missed:
        print(f"\nMissed bots ({len(missed)}):")
        for uid in sorted(missed):
            print(f"  {uid}")

    # Show false positives
    false_pos = detected - true_bots
    if false_pos:
        print(f"\nFalse positives ({len(false_pos)}):")
        for uid in sorted(false_pos):
            print(f"  {uid}")

    return {"tp": tp, "fp": fp, "fn": fn, "score": score, "max": max_score}


# ============================================================================
# OUTPUT: Write detection file in competition format
# ============================================================================

def write_detections(detections, output_path):
    """Write detected bot IDs to a file, one per line."""
    with open(output_path, "w") as f:
        for uid in sorted(detections):
            f.write(uid + "\n")
    print(f"\nDetections written to {output_path} ({len(detections)} accounts)")


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Bot or Not Challenge â€” Bot Detection Pipeline"
    )
    parser.add_argument(
        "dataset",
        nargs="?",
        default="all",
        help=(
            "Path to dataset_posts_users.json file (or alias: 30/32/all). "
            "Default: all (runs both dataset 30 and 32)"
        ),
    )
    parser.add_argument(
        "--threshold", "-t",
        type=int,
        default=DETECTION_THRESHOLD,
        help=f"Detection score threshold (default: {DETECTION_THRESHOLD})",
    )
    parser.add_argument(
        "--bots", "-b",
        help=(
            "Path to dataset.bots.txt for evaluation (optional). "
            "If omitted, auto-uses matching dataset.bots.<id>.txt when possible"
        ),
    )
    parser.add_argument(
        "--output", "-o",
        help="Output file path for detections (default: detections.txt)",
        default="detections.txt",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Print per-user scoring details",
    )

    args = parser.parse_args()

    dataset_arg = str(args.dataset).strip().lower()
    if dataset_arg in {"all", "both"}:
        dataset_paths = [
            str(DATASET_POSTS_USERS_30),
            str(DATASET_POSTS_USERS_32),
        ]
    else:
        dataset_paths = [_resolve_dataset_alias(args.dataset)]

    run_summaries = []

    for dataset_path in dataset_paths:
        print("\n" + "#" * 60)
        print(f"RUNNING DATASET: {Path(dataset_path).name}")
        print("#" * 60)

        # Run detection
        detections, all_scores = detect_bots(
            dataset_path,
            threshold=args.threshold,
            verbose=args.verbose,
        )

        # Write output (dataset-specific names when running multiple datasets)
        output_path = args.output
        if len(dataset_paths) > 1:
            output_path = _output_path_for_dataset(args.output, dataset_path)
        write_detections(detections, output_path)

        # Evaluate if ground truth is provided/available
        bots_path = args.bots or _default_bots_for_dataset(dataset_path)
        eval_result = None
        if bots_path:
            eval_result = evaluate(detections, bots_path)

        run_summaries.append(
            {
                "dataset": Path(dataset_path).name,
                "users": len(all_scores),
                "detections": len(detections),
                "eval": eval_result,
            }
        )

    if len(run_summaries) > 1:
        print("\n" + "=" * 60)
        print("FINAL SUMMARY (ALL DATASETS)")
        print("=" * 60)

        total_detections = sum(r["detections"] for r in run_summaries)
        total_users = sum(r["users"] for r in run_summaries)
        print(f"Total detections:      {total_detections}")
        print(f"Total users scanned:   {total_users}")
        print(f"Overall detection rate:{(total_detections / total_users):.1%}")

        eval_runs = [r for r in run_summaries if r["eval"] is not None]
        if eval_runs:
            total_tp = sum(r["eval"]["tp"] for r in eval_runs)
            total_fp = sum(r["eval"]["fp"] for r in eval_runs)
            total_fn = sum(r["eval"]["fn"] for r in eval_runs)
            total_score = sum(r["eval"]["score"] for r in eval_runs)
            total_max = sum(r["eval"]["max"] for r in eval_runs)
            precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
            recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0

            print(f"Combined TP/FP/FN:     {total_tp} / {total_fp} / {total_fn}")
            print(f"Combined Precision:    {precision:.1%}")
            print(f"Combined Recall:       {recall:.1%}")
            print(
                f"Combined Score:        {total_score:+d} / {total_max} "
                f"({(total_score / total_max):.0%} of perfect)"
            )

        print("=" * 60)


if __name__ == "__main__":
    main()